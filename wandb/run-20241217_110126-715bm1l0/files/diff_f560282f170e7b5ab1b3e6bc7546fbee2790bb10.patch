diff --git a/model.zip b/model.zip
deleted file mode 100644
index 0e3dbc5..0000000
Binary files a/model.zip and /dev/null differ
diff --git a/models/DDPG/model.zip b/models/DDPG/model.zip
deleted file mode 100644
index 0cc4cb3..0000000
Binary files a/models/DDPG/model.zip and /dev/null differ
diff --git a/models/PPO/model.zip b/models/PPO/model.zip
deleted file mode 100644
index 8a0582d..0000000
Binary files a/models/PPO/model.zip and /dev/null differ
diff --git a/models/SAC/model.zip b/models/SAC/model.zip
deleted file mode 100644
index 0e3dbc5..0000000
Binary files a/models/SAC/model.zip and /dev/null differ
diff --git a/quad_env.py b/quad_env.py
index c78236b..b0e8033 100644
--- a/quad_env.py
+++ b/quad_env.py
@@ -173,21 +173,21 @@ class QuadEnv(MujocoEnv):
         """
         forward_reward = x_velocity * self.FORWARD_REWARD_WEIGHT
         healthy_reward = self.healthy_reward
-        vel_reward = self.reward_tracking_lin_vel(commands=action,
-                                                  x_velocity=x_velocity,
-                                                  y_velocity=y_velocity)
+        # vel_reward = self.reward_tracking_lin_vel(commands=action,
+        #                                           x_velocity=x_velocity,
+        #                                           y_velocity=y_velocity)
 
-        rewards = forward_reward + healthy_reward + vel_reward
+        rewards = forward_reward + healthy_reward
 
         ctrl_cost = self.control_cost(action)
         contact_cost = self.contact_cost
-        costs = ctrl_cost + contact_cost
+        costs = ctrl_cost
 
         reward = rewards - costs
 
         reward_info = {"reward_forward": forward_reward,
                        "reward_ctrl": -ctrl_cost,
-                       "reward_contact": -contact_cost,
+                       "reward_contact": 0,
                        "reward_survive": healthy_reward}
 
         return reward, reward_info
diff --git a/run_experiments.py b/run_experiments.py
index 600b493..950094b 100644
--- a/run_experiments.py
+++ b/run_experiments.py
@@ -5,17 +5,17 @@ from stable_baselines3.common.monitor import Monitor
 import wandb
 
 NUM_EPISODES = 10000
-NUM_TIME_STEPS = 5000000
-GRADIENT_SAVE_FREQ = 100
+NUM_TIME_STEPS = 1000000
+GRADIENT_SAVE_FREQ = 500
 
 ENV_PARAMETERS = {
-    'FORWARD_REWARD_WEIGHT': 1,
+    'FORWARD_REWARD_WEIGHT': 1.0,
     'TRACKING_REWARD_WEIGHT': 0.5,
-    'CONTROL_COST_WEIGHT': 0.05,
+    'CONTROL_COST_WEIGHT': 0.5,
     'CONTACT_COST_WEIGHT': 5e-4,
     'HEALTHY_REWARD_WEIGHT': 1,
     'MAIN_BODY': 1,
-    'HEALTHY_Z_RANGE': (0.195, 0.75),
+    'HEALTHY_Z_RANGE': (0.2, 1),
     'RESET_NOISE_SCALE': 0.1,
     'CONTACT_FORCE_RANGE': (-1.0, 1.0)
 }
@@ -76,7 +76,7 @@ def training_initialization(algorithm_model):
         callback=WandbCallback(
             verbose=1,
             gradient_save_freq=GRADIENT_SAVE_FREQ,
-            model_save_path=f'models/{algorithm_model.__name__}'
+            model_save_path=f'training_outputs/{algorithm_model.__name__}'
         ))
 
     return model, vec_env, run, obs
diff --git a/runs/DDPG/DDPG_1/events.out.tfevents.1733158002.pc040-ubu.150329.2 b/runs/DDPG/DDPG_1/events.out.tfevents.1733158002.pc040-ubu.150329.2
deleted file mode 100644
index 2fe2b14..0000000
Binary files a/runs/DDPG/DDPG_1/events.out.tfevents.1733158002.pc040-ubu.150329.2 and /dev/null differ
diff --git a/runs/PPO/PPO_1/events.out.tfevents.1733157734.pc040-ubu.150329.0 b/runs/PPO/PPO_1/events.out.tfevents.1733157734.pc040-ubu.150329.0
deleted file mode 100644
index 3200e12..0000000
Binary files a/runs/PPO/PPO_1/events.out.tfevents.1733157734.pc040-ubu.150329.0 and /dev/null differ
diff --git a/runs/SAC/SAC_1/events.out.tfevents.1733157909.pc040-ubu.150329.1 b/runs/SAC/SAC_1/events.out.tfevents.1733157909.pc040-ubu.150329.1
deleted file mode 100644
index 67f70f3..0000000
Binary files a/runs/SAC/SAC_1/events.out.tfevents.1733157909.pc040-ubu.150329.1 and /dev/null differ
diff --git a/simulate_environment.py b/simulate_environment.py
index 6e7c8aa..7cdce0a 100644
--- a/simulate_environment.py
+++ b/simulate_environment.py
@@ -1,28 +1,36 @@
 import mujoco
 import mujoco.viewer
-from stable_baselines3 import SAC
+from stable_baselines3 import PPO, SAC, DDPG
 import numpy as np
 
-rl_model = SAC.load("model.zip", print_system_info=True)
+ppo = PPO.load("models/PPO/v0/model.zip")
+sac = SAC.load("models/SAC/v0/model.zip")
+ddpg = DDPG.load("models/DDPG/v0/model.zip")
+
+algorithm = ppo
 
 xml_path = "robot/anybotics_anymal_c/scene.xml"
 
 mujoco_model = mujoco.MjModel.from_xml_path(xml_path)
 data = mujoco.MjData(mujoco_model)
 
+
+def get_obs():
+    MIN_CONTACT_FORCE = -1.0
+    MAX_CONTACT_FORCE = 1.0
+    raw_contact_forces = data.cfrc_ext
+    contact_forces = np.clip(raw_contact_forces,
+                             MIN_CONTACT_FORCE,
+                             MAX_CONTACT_FORCE)
+    new_contact_forces = contact_forces[1:].flatten()
+    obs = np.concatenate([data.qpos[2:], data.qvel, new_contact_forces])
+    return obs
+
+
 with mujoco.viewer.launch_passive(mujoco_model, data) as viewer:
     while viewer.is_running():
-        MIN_CONTACT_FORCE = -1.0
-        MAX_CONTACT_FORCE = 1.0
-        raw_contact_forces = data.cfrc_ext
-        contact_forces = np.clip(raw_contact_forces,
-                                 MIN_CONTACT_FORCE,
-                                 MAX_CONTACT_FORCE)
-        new_contact_forces = contact_forces[1:].flatten()
-        obs = np.concatenate([data.qpos[2:], data.qvel, new_contact_forces])
-        action, _states = rl_model.predict(obs, deterministic=True)
-        data = mujoco.MjData(mujoco_model)
-        # print(data.cvel)
+        obs = get_obs()
+        action, _states = algorithm.predict(obs, deterministic=True)
         with viewer.lock():
             data.ctrl[:] = action
         mujoco.mj_step(mujoco_model, data)
diff --git a/wandb/latest-run b/wandb/latest-run
index 3016979..c12805d 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20241202_140508-rvzrzj9r
\ No newline at end of file
+run-20241217_110126-715bm1l0
\ No newline at end of file
