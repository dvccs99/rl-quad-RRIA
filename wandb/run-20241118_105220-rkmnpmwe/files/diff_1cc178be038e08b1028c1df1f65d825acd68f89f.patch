diff --git a/custom_env.py b/custom_env.py
index 24e0074..bdb99fd 100644
--- a/custom_env.py
+++ b/custom_env.py
@@ -157,7 +157,7 @@ class QuadEnv(MujocoEnv):
         }
 
         if self.render_mode == "human":
-            self.render()
+            self.render(camera='track')
         return observation, reward, terminated, truncated, info
 
     def __get_rew(self, x_velocity: float, action) -> tuple:
diff --git a/first_run.py b/first_run.py
index dd9e67a..a7633a5 100644
--- a/first_run.py
+++ b/first_run.py
@@ -1,5 +1,10 @@
 from custom_env import QuadEnv
 from stable_baselines3 import SAC
+from wandb.integration.sb3 import WandbCallback
+from stable_baselines3.common.monitor import Monitor
+from stable_baselines3.common.vec_env import VecVideoRecorder
+import wandb
+
 
 env_parameters = {
     'forward_reward_weight': 1,
@@ -8,29 +13,51 @@ env_parameters = {
     'healthy_reward_weight': 1,
     'main_body': 1,
     'healthy_z_range': (0.195, 0.75),
-    'include_cfrc_ext_in_observation': True,
-    'terminate_when_unhealthy': True,
     'reset_noise_scale': 0.1,
     'contact_force_range': (-1.0, 1.0)
 }
 
 mujoco_parameters = {
     'xml_file': './robots/boston_dynamics_spot/scene.xml',
-    'frame_skip': 5,
+    'frame_skip': 50,
     'observation_space': None,  # needs to be defined after
     'default_camera_config': 'default_camera_config',
-    'render_mode': 'human'
+    'render_mode': 'rgb_array'
 }
 
 env = QuadEnv(env_parameters, mujoco_parameters)
+env = Monitor(env)
+
+run = wandb.init(
+    project="Quad_Mujoco",
+    sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics
+    monitor_gym=True,  # auto-upload the videos of agents playing the game
+    save_code=True,  # optional
+)
 
-model = SAC("MlpPolicy", env, verbose=1)
-model.learn(total_timesteps=2000)
+model = SAC("MlpPolicy",
+            env,
+            verbose=1,
+            tensorboard_log=f"runs/{run.id}")
 
 vec_env = model.get_env()
+# vec_env = VecVideoRecorder(vec_env,
+#                            f"videos/{run.id}",
+#                            record_video_trigger=lambda x: x % 2000 == 0,
+#                            video_length=250,)
+
+model.learn(
+    total_timesteps=10000,
+    callback=WandbCallback(
+        gradient_save_freq=100,
+        model_save_path='models'
+    ))
+
+
 obs = vec_env.reset()
 for i in range(10000):
     action, _state = model.predict(obs, deterministic=True)
     obs, reward, done, info = vec_env.step(action)
-    vec_env.render("human")
+    vec_env.render('human')
 env.close()
+run.finish()
diff --git a/robots/boston_dynamics_spot/scene.xml b/robots/boston_dynamics_spot/scene.xml
index 35d863b..3d16d59 100644
--- a/robots/boston_dynamics_spot/scene.xml
+++ b/robots/boston_dynamics_spot/scene.xml
@@ -20,10 +20,4 @@
   <worldbody>
     <geom name="floor" size="0 0 0.05" type="plane" material="groundplane"/>
   </worldbody>
-
-  <worldbody>
-    <geom name="red_box" type="box" pos = "2. .0 .0" size=".5 .5 .2" rgba="1 0.5 0 1"/>
-    <geom name="red_box_2" type="box" pos = "2.5 .0 .0" size=".5 .5 .4" rgba="1 0.5 0 1"/>
-    <geom name="red_box_3" type="box" pos = "3. .0 .0" size=".5 .5 .6" rgba="1 0.5 0 1"/>
-  </worldbody>
 </mujoco>
